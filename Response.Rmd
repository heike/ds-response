---
title: "Response to "
author: "Heike Hofmann & Susan VanderPlas"
output:
  pdf_document: default
bibliography: refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```


# TODO
- Package Refs
- 

<!--# Introduction-->
David Donoho's "50 years of Data Science" provides a valuable perspective on the statistics-vs-data science debate that has been raging in academic statistics departments over the past couple of years. The debate about the relative merits of theoretical and applied statistics flares up occasionally, and even in the infancy of statistics as a discipline distinct from mathematics, there was "something slightly disreputable about mathematical statistics" because of its applied nature [@salsburg2001lady, pp. 208]. It seems, however, that we may be witnessing the birth of the academic discipline of data science as a separate entity from statistics. While data science itself has been, according to Donoho, around for 50 years or more, academic initiatives focusing on the practice of data analysis are becoming ever more popular.

<!--# Historical Parallels -->
## Historical Parallels
Statistics became an academic discipline separate from mathematics in part due to the focus on problems of a more applied nature that involved real-world data. In the US, the first stand-alone statistics entity was Iowa State University's Agricultural and Statistical Laboratory, founded by George Snedecor in 1933, and was focused primarily on statistical analyses of agricultural data. A particularly illustrative paragraph from A Lady Tasting Tea [@salsburg2001lady, pp 216] illustrates the development of statistics as separate from mathematics and lays the foundation for the current situation: 

> The mathematics departments of American and European universities missed the boat when mathematical statistics arrived on the scene. With Wilks leading the way, many universities developed separate statistics departments. The mathematics departments missed the boat again when the digital computer arrived, disdaining it as a mere machine for doing engineering calculations. Separate computer science departments arose, some of them spun off from engineering departments, others spun off from statistics departments. The next big revolution that involved new mathematical ideas was the development of molecular biology in the 1980s. As we shall see in chapter 28, both the mathematics and the statistics departments missed that particular boat.

The parallels are obvious; as statistics has matured as a discipline, researchers have specialized, focusing on the practice of data analysis or on the minutiae of theoretical underpinnings of statistical techniques. Those in the first group are beginning to call themselves "Data Scientists", while those in the second group continue to refer to themselves as "Statisticians". 

Another parallel can be drawn between the beginning of academic statistics and the beginning of academic data science. As statistics developed as an academic discipline distinct from mathematics, there were many new tools and techniques for analyzing data: ANOVA, least-squares regression, Box-Cox transformations, etc. These tools were applied to problems of the day, and as the field matured, some of the tools were replaced by more technologically sophisticated techniques. Similarly, there are a flurry of tools for doing data science which are currently in vogue (`rmarkdown`, the tidyverse, `caret` by @caret, `hadoop`, `Apache Spark`), some of which are built on fundamentally new approaches to data analysis and some that will be supplanted as technology changes. 

<!-- May want to move this into the research/tools section? -->

Some of the popularity of recently developed tools in R, such as `rmarkdown` [@rmarkdown] or `tidyverse` [@tidyverse], can be explained because these tools are actual implementations of deeper concepts. Both of these tools can be said to make data analysis an application of literate programming. Literate programming is an idea proposed first by Donald Knuth [@knuth1984literate] and implemented in his  `WEB` system. Literate programming is the idea of interweaving code and text into a single document while also aiming for highly modular and therefore re-usable code pieces. While `rmarkdown` allows us the first, `tidyverse` is a collection of highly modular R packages that all adhere to a similar API, which allows `plug and play` data manipulation, modelling, and visualization. Identifying these kind of concepts and generalizable frameworks are part of what drives further research in Data Science.

Donoho should give himself more credit - he has been at the fore-front of the concepts which are now fundamental building blocks in the Science of DS, such as the issue of **reproducibility of research**. Back in 1995  @donoho:1995 phrased the main idea of computational reproducibility (as opposed to experimental reproducibility, which, of course is also important, but cannot be ensured by any computational tools):

> When we publish articles containing figures which were generated by computer, we also publish the complete software environment which generates the figures.

@marwick:2016 defines the "Pi-shaped researcher" as a researcher who in addition to the domain knowledge of the field also knows about and implements best practices of reproducibility. One way to view the divergence between statistics and data science is to say that data science encompasses more computational and literate programming style reproducibility, with a focus on practice as well as theory. 

## The Practice of Data Science
<!-- Now for the critical part -->
The practice of data science cannot be easily separated from research into data science and teaching data science skills. Often, there is a cycle, where an idea develops as a solution to solve a practical problem, then is extended and applied to a wider set of problems through research, and is finally taught to new practitioners of data science, who then develop new tools. This cycle of practice, research, teaching, and practice can be applied to several distinct areas of data science. 

Rather than dividing data science into six divisions, we would suggest that there are 6 steps in any data analysis, and thus six parts of data science:

1. Data Provenance
2. Data Exploration and Preparation
3. Data Representation and Transformation
4. Computing with Data
5. Data Modeling
6. Communication of Results

5 of these steps roughly concur with the divisions of data science in Donoho's article; we have exchanged data provenance for "Science about Data Science". We have also rephrased "Data Visualization and Presentation" as "Communication of Results". Just as data exploration without visualization is unthinkable, presentation of results are much harder without visualizations. Explicitly mentioning visualization in one but not the other part might lead to the (superficial) impression that data visualization does not start until point 6 in the process.

Each of these steps in a data analysis can be practiced, researched, and taught. Tool development for data science often flows from practice to research and then is taught to new individuals; occasionally, tools flow from teaching to practice to research instead. Several of Jenny Bryan's packages appear to have been inspired by differences in workflow between data scientists and collaborators from other areas (`googlesheets` by @googlesheets, [`linen`](https://github.com/rsheets/linen), [`jailbreakr`](https://github.com/rsheets/jailbreakr)), while other packages seem to flow from experience teaching data science ([`githug`](https://github.com/jennybc/githug)). 


An academic data scientist or applied statistician should be focused both on education and tool creation to facilitate using new approaches to work with data. The inspiration for new theories and software can flow from both educational contexts and from practical projects; thus, both sources should be respected and encouraged. Importantly, though, tool creation is a parallel path that exists throughout the 6 steps for the practice of data science: it is integral to the process of data science, and critical for the evolution of the field as a whole. Most people who program develop convenience functions to complete oft-repeated tasks; it is when those functions are expanded and shared with others that they pollinate the community. R packages such as `dplyr` [@dplyr], `tidyr` [@tidyr], and `rmarkdown` are iterations of previous packages and software (`reshape` and `reshape2` by @reshape, `plyr` by @plyr, `knitr` by @knitr, @knitr2 and `Sweave` by @sweave), and all were developed to solve practical problems that became obvious through teaching or data analysis.

<!-- 
- Focus for academic statisticians should be jointly on tool creation and education
- Tool creation isn't a separate "6th Division of data science" - tool creation is a companion to every other division of data science. It is integral to the process!
- Call out people who are doing both data science and tool creation. 
-->
<!--
- Discussion of prediction vs. inference
    - shouldn't swing too far to one side or another - in industry, models still have to be explained and justified to management. It's critical to know when the model is likely to be wrong or less trustworthy. 
    - Traditional statistics tends to focus on inference and use prediction as a goal to fit a model so that parameter values can be analyzed and inferred from that model. (or at least, parametric statistics seems that way).
-->

## Prediction and Inference

In the discussion of Two Cultures of Statistics, Donoho summarizes Breiman's claims that there are two approaches to extracting information from data: Prediction, and Inference. It is true that statisticians focus heavily on parametric inference when teaching techniques; predictions are interpreted within the context of the generating parametric model. @harville:2014 discusses this paradigm by splitting prediction even further into model-based approaches to prediction and algorithmic approachess to prediction, favored by statisticians and computer scientists respectively.  Data science does not have the same bias against algorithmic approaches to prediction seen in statistics departments, but what is unclear is whether one approach is superior to the other in applications. We find Harville's distinction between algorithmic and model-based prediction to be more useful in understanding the divide between traditional statistics and data science approaches to modeling. 

Donoho suggests that by combining predictive inference with the Common Task Framework, it is simple to obtain iterative improvements in prediction accuracy. Donoho's discussion of the CTF is certainly useful, but it is entirely possible to apply the CTF to model-based predictions as well as to algorithmic predictions. There is no inherent conflict between the CTF and approaches favored by traditional statisticians. Rather, the philosophical approach to the problem produces this underlying conflict: without a model and parameters that can be interpreted, how does one identify the strengths and weaknesses of the predictions? It is much harder to communicate the results to managers and businesspeople who must act on the predictions from an algorithmic model. The algorithmic approach requires a "leap of faith" that depends heavily on the culture of the "customer" seeking help from the data scientist. In many corporate environments, the black box of a neural network or other machine learning approach is less likely to gain management buy-in than an equation, even if the equation is complex and intimidating. In other organizations, it is preferrable to use the en-vogue tool, which is often an algorithmic model (neural networks, random forests, support vector machines, etc.), and there is little desire to examine the underlying mechanisms. This dichotomy is company-culture and domain specific: it is often not possible to find underlying causal mechanisms in data collected outside the confines of a designed experiment (for instance, in social media logs), so the disadvantages of algorithmic models are not problematic. In disciplines which deal with more concrete phenomena (engineering, manufacturing, insurance), model-based approaches are preferrable because it is important to understand why the model makes certain predictions and what factors are most influential. Students must learn both approaches to be successful in a wide range of applications of data science.  


## Teaching of DS
Donoho suggests that teaching data science should be focused on the 6 branches of data science: We must teach not just data modeling, but also data exploration, preparation, computing, and visualization. He suggests using two baseball-focused resources to accomplish this task, and while we applaud the use of real-life data, it is important to point out that students who are not interested in baseball might get the impression that baseball is all there is to statistics. This does an incredible disservice to the statistical community. When one of the authors took linear models, all of the material was discussed in the context of agricultural field experiments, and students with no interest in corn and soybean plots had a much harder time relating to the fundamental concepts. In any course, it is important to provide diversity! Courses should use data from a variety of real life sources: baseball, but also social network data, historical data from the US Census, data from experiments conducted at the university (including data from split plot agricultural experiments), and many other sources. 

What is important is that data used in statistics and data science courses should be messy, requiring students to grapple not only with the statistical methods and models which are applied to the data but also with data cleaning and tools used to accomplish this task. While this approach requires more of an investment (for instance, students may need to be exposed to packages like `stringr` along with tools for modeling statistical networks), it promises to produce students who are fully able to handle real-world data and are thus capable of working in academia or in industry right out of school. In addition, exposing students to new software packages in parallel with new statistical concepts allows them to practice the same skills that are necessary to adapt to the ever-changing set of tools needed to do analysis in the business world. 

The Park City Math Institute (PCMI) undergraduate faculty group released a set of curriculum guidelines for undergraduate data science programs [@pcmi] which concurs with our suggestions as well as Donoho's push for teaching all 6 parts of data science. The PCMI guidelines emphasize the importance of teaching students to obtain, wrangle, curate, manage, process, explore, define, analyze, and communicate data - that is, they say that data should be at the center of all data science courses, and that raw data should be included in the teaching process. They suggest that the guiding principles of a data science program are:

1. Data Science as a science
2. Data Science as an Interdiciplinary field
3. Data at the core: classes should require students to work with the whole range of data-related tasks
4. Analytical (Computational and Statistical) thinking
5. Mathematical Foundations
6. Flexibility

These foundational principles are entirely in line with the six parts of data science we have proposed above, modified from Donoho's divisions of data science. Students who cannot think scientifically cannot adequately perform data analyses; neither can students who do not have a set of interdisciplinary skills ranging from programming to mathematics to communication. Students must be provided the opportunity to learn not only the algorithms and statistical foundations of data science, but a whole range of practical skills, and data science programs must be flexible enough to allow students and faculty to specialize in one or more areas related to the acquisition, processing, analysis, modeling, and visualization of data. 


<!-- Need some sort of conclusion... -->
As useful as it is to have foundational principles for data science, we must be careful not to set up barriers to exclude individuals who are computing with data. Any applied statistician is already a data scientist, but there are also data scientists in computer science, psychology, medicine, bioinformatics, and a whole host of other disciplines. Whatever philosophical disagreements may arise in the next 50 years, we should be careful to learn from the past, preserving the "big tent" approach that the discipline developed organically. Data Science is here to stay, and statisticians should welcome the additional energy and opportunities for collaboration that it has brought to the field. 

# References
