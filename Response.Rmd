---
title: "Response to "
author: "Heike Hofmann & Susan VanderPlas"
output:
  html_document: default
bibliography: refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```


<!--# Introduction-->
David Donoho's "50 years of Data Science" provides a valuable perspective on the statistics-vs-data science debate that has been raging in academic statistics departments over the past couple of years. The debate about the relative merits of theoretical and applied statistics flares up occasionally, and even in the infancy of statistics as a discipline distinct from mathematics, there was "something slightly disreputable about mathematical statistics" because of its applied nature [@salsburg2001lady, pp. 208]. It seems, however, that we may be witnessing the birth of the academic discipline of data science as a separate entity from statistics. While data science itself has been, according to Donoho, around for 50 years or more academic initiatives focusing on the practice of data analysis are becoming ever more popular.

<!--# Historical Parallels -->
## Historical Parallels
Statistics became an academic discipline separate from mathematics in part due to the focus on problems of a more applied nature that involved real-world data. In the US, the first stand-alone statistics entity was Iowa State University's Agricultural and Statistical Laboratory, founded by George Snedecor in 1933, and was focused primarily on statistical analyses of agricultural data. A particularly illustrative paragraph from A Lady Tasting Tea [@salsburg2001lady, pp. 216] illustrates the development of statistics as separate from mathematics and lays the foundation for the current situation: 

> The mathematics departments of American and European universities missed the boat when mathematical statistics arrived on the scene. With Wilks leading the way, many universities developed separate statistics departments. The mathematics departments missed the boat again when the digital computer arrived, disdaining it as a mere machine for doing engineering calculations. Separate computer science departments arose, some of them spun off from engineering departments, others spun off from statistics departments. The next big revolution that involved new mathematical ideas was the development of molecular biology in the 1980s. As we shall see in chapter 28, both the mathematics and the statistics departments missed that particular boat.

The parallels are obvious; as statistics has matured as a discipline, researchers have specialized, focusing on the practice of data analysis or on the minutiae of theoretical underpinnings of statistical techniques. Those in the first group are beginning to call themselves "Data Scientists", while those in the second group continue to refer to themselves as "Statisticians". 

Another parallel can be drawn between the beginning of academic statistics and the beginning of academic data science. As statistics developed as an academic discipline distinct from mathematics, there were many new tools and techniques for analyzing data: ANOVA, least-squares regression, Box-Cox transformations, etc. These tools were applied to problems of the day, and as the field matured, some of the tools were replaced by more technologically appropriate techniques. Similarly, there are a flurry of tools for doing data science which are currently in vogue (`knitr`, the tidyverse, `CARAT`, `hadoop`), some of which contain lasting approaches to data analysis and some that will be supplanted as technology changes. 

<!-- May want to move this into the research/tools section? -->
Some of the popularity of recently developed tools in R, such as `knitr` or `tidyverse`, can be explained that these tools are actual implementations of deeper concepts. Both of these developments are a step into the direction of making data analysis an application of literate programming. Literate programming is an idea proposed first by Donald Knuth [@knuth1984literate] and implemented in his  `WEB` system. Literate programming is the idea of interweaving code and text into a single document while also aiming for highly modular and therefore re-usable code pieces. While `knitr` allows us the first, `tidyverse` is a collection of highly modular R packages that all adhere to a similar API, which allows `plug and play` data manipulation, modelling, and visualization. Identifying these kind of concepts and generalizable frameworks are part of what drives further research in Data Science.

## The Practice of Data Science
<!-- Now for the critical part -->
The practice of data science cannot be easily separated from research into data science and teaching data science skills. Often, there is a cycle, where an idea is created to solve a practical problem, then extended and applied to a wider set of problems through research, and is finally taught to new practitioners of data science, who then develop new tools. This cycle of practice, research, teaching, and practice can be applied to several distinct areas of data science. 

Rather than dividing data science into six divisions, we would suggest that there are 6 steps in any data analysis, and thus six parts of data science:

1. Data Provenance
2. Data Exploration and Preparation
3. Data Representation and Transformation
4. Computing with Data
5. Data Modeling
6. Communication of Results

5 of these steps roughly concur with the divisions of data science in Donoho's article; we have exchanged data provenance for "Science about Data Science". We have also rephrased "Data Visualization and Presentation" as "Communication of Results". Just as data exploration without visualization is unthinkable, presentation of results are much harder without visualizations. Explicitly mentioning visualization in one but not the other part might lead to the (superficial) impression that data visualization does not start until point 6 in the process.

Each of these steps in a data analysis can be practiced, researched, and taught. Tool development for data science often flows from practice to research and then is taught to new individuals; occasionally, tools flow from teaching to practice to research instead. Several of Jenny Bryan's packages appear to have been inspired by differences in workflow between data scientists and collaborators from other areas (googlesheets, linen, jailbreakr), while other packages seem to flow from experience teaching data science (githug). 


- Focus for academic statisticians should be jointly on tool creation and education
- Tool creation isn't a separate "6th Division of data science" - tool creation is a companion to every other division of data science. It is integral to the process!
- Call out people who are doing both data science and tool creation. 

- Discussion of prediction vs. inference
    - shouldn't swing too far to one side or another - in industry, models still have to be explained and justified to management. It's critical to know when the model is likely to be wrong or less trustworthy. 
    - Traditional statistics tends to focus on inference and use prediction as a goal to fit a model so that parameter values can be analyzed and inferred from that model. (or at least, parametric statistics seems that way).

In the discussion of Two Cultures of statistics, Donoho summarizes Breiman's claims that there are two approaches to extracting information from data: Prediction, and Inference. It is true that statisticians focus heavily on parametric inference when teaching techniques; predictions are interpreted within the context of the generating parametric model. Harville [@harville:2014] discusses this paradigm by splitting prediction even further into model-based approaches to prediction and algorithmic approachess to prediction, favored by statisticians and computer scientists respectively.  Data science does not have the same bias against algorithmic approaches to prediction seen in statistics departments, but what is unclear is whether one approach is superior to the other in applications. We find Harville's distinction between algorithmic and model-based prediction to be more useful in understanding the divide between traditional statistics and data science approaches to modeling. 

Donoho suggests that by combining predictive inference with the Common Task Framework, it is simple to obtain iterative improvements in prediction accuracy. Donoho's discussion of the CTF is certainly useful, but it is entirely possible to apply the CTF to model-based predictions as well as to algorithmic predictions. There is no inherent conflict between the CTF and approaches favored by traditional statisticians. Rather, the philosophical approach to the problem produces this underlying conflict: without a model and parameters that can be interpreted, how does one identify the strengths and weaknesses of the predictions? It is much harder to communicate the results to managers and businesspeople who must act on the predictions from an algorithmic model. The algorithmic approach requires a "leap of faith" that depends heavily on the culture of the "customer" seeking help from the data scientist. In many corporate environments, the black box of a neural network or other machine learning approach is less likely to gain management buy-in than an equation, even if the equation is complex and intimidating. 


## Teaching of DS

not just football or baseball, please. What is laudable, though, is that these are real-life data. 

Teaching DS: PCMI guidelines

# References
